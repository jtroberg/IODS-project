# Week 5


This week we have used the data from United Nations Development Programme, Human Development Reports ([hdr.undp.org](http://hdr.undp.org/en/content/human-development-index-hdi)). 

From the reports, the two data sets of Human Development Index (HDI) and Gender Inequality Index were downloaded, combined, and some data variables were selected for this week's analysis exercise.

```{r human data}
# Load the human.txt data into R.

human<-read.table("~/Desktop/IODS-project/data/human.txt", sep="", header=T)

# Explore the structure and the dimensions of the data 
str(human)
dim(human)
```

In our data set we have 155 objects and 8 variables, so we have 155 rows (different countries) and 8 columns. All the variables are recognized as numerical, most of them have decimal values, but the GNI and Mat.Mor are integers. The chosen 8 variables (column heading abbreviations) from larger data set describe following things:


1. The proportion of females divided with proportion of males with at least secondary education (Edu2.FM).
2. The proportion of females divided with the proportion of males in the labour force. (Labo.FM)
3. Expected years of schooling for children of school entering age (Edu.Exp)
4. Life expectancy at birth (Life.Exp)
5. Gross national income per capita (GNI) 
6. Maternal mortality ratio (Mat.Mor)
7. Adolescent birth rate (Ado.Birth)
8. Percentage of female representatives in parliament (Parli.F)

```{r graphical overview}
# Show a graphical overview of the data
# Access to ggplot2 and tidyr
library(ggplot2)
library(tidyr)

# For easier overview of the data, the data set is scaled and class changed from numeric to data frame.
human_scaled<-scale(human)
human_scaled<-as.data.frame(human_scaled)

# Next scaled data is gathered to key-value pairs and plotted for visualization of the data.
gather(human_scaled)%>%ggplot(aes(value))+facet_wrap("key", scales="free_y", )+geom_histogram(bins=155, col="darkblue")+xlim(-6,6)

# To see how scaling affected another histograms are made for the original data.
gather(human)%>%ggplot(aes(value))+facet_wrap("key", scales="free", )+geom_histogram(bins=155, col="darkred")


```

From the blue histograms (scaled values) it is perhaps easier to see, which variables are normally distributed, and which are not. Ado.Birth, GNI and Mat.Mor are not following the normal distribution. From red histograms (not-scaled histograms) it is obvious that data set doesn't have negative values, which makes sense, because none of the variables in the data table can have negative value. 

The summary tables below are first for not-scaled data and the next one is for the scaled data. These two tables (and histograms above) show that largest distribution is with GNI variable and least distributed is Life.Exp.

```{r summary tables}
# Creating summary tables for both scaled and original human data
summary(human)
summary(human_scaled)
```

The relationships between variables can be visualized by creating correlation plot. In this first correlation plot below, the distribution of the data variables is again visible in the middle of the plot (from higher left to lower bottom). The highest correlation value can be seen between variables Life.Exp and Edu.Exp. This is positively correlated, so in countries where expected years of schooling is high, the life expectancy is high also. Negatively correlated is the Mat.Mor and Life.Exp, so in those places, where life expectancy is high, the maternal mortality ratio is low. Least correlated were Edu2.FM and Labo.FM, so the proportion of females divided with proportion of males with at least secondary education and the proportion of females divided with the proportion of males in the labour force were not correlated. The same observations can be seen from the second correlation plot below.

```{r correlation plots}
# Access GGally and corrplot
library(GGally)
library(corrplot)
# Visualize data variables with ggpairs()
ggpairs(human)

# Compute correlation matrix and visualize it with corrplot()
cor(human)%>%corrplot(order ="hclust", method="color", type="lower")

```

The not standardized human data is then analyzed with PCA (principal component analysis). The variability of this data is almost completely from component 1 (99.99% of variability) and the second component (orthogonal to PC1) makes the 0.01% variation in the data. The other principal components don't have importance in terms of captured variance. So, we have the two principal components (uncorrelated variables) which will capture the highest variation. The angles of arrows, however, are all looking the same, so this is not giving us much information about the correlation, because all the arrows are small angle with PC1 axis, meaning that the correlation is high between the features and PC1. The length of the arrow represents the standard deviation of the feature. 

```{r PCA on the not standardized data}
# Perform principal component analysis on the not standardized human data (with the SVD method)
pca_human<-prcomp(human)

# Create and print summary of pca_human
s<-summary(pca_human)
s

# Rounded percentages of variance captured by each PC, print them out
pca_pr<-round(1*s$importance[2,]*100, digits=2)
pca_pr

# Create object pc_lab to be used as axis labels
pc_lab<-paste0(names(pca_pr), "(", pca_pr, "%)")

# Draw a biplot (and suppress warnings for zero-lenght arrows)
suppressWarnings(biplot(pca_human, cex=c(0.8,1), col=c("grey40", "deeppink2"), xlab=pc_lab, ylab=NA, main="Biplot using not standardized human data"))

```

Next the PCA is done for the standardized variables. Now we can get more principal components. Comparing not-standardized and standardized PCAs, it is clear that PCA is sensitive to the scaling of original values, assuming that features that have larger variance are more important. From this standardized biplot it is clearly visible that Labo.FM and Parli.F are their own group, as well Mat.Mor and Ado.Birth are, and all the rest 4 variables are in the grouped on the left side of this biplot. So this means that maternal mortality ratio and 
adolescent birth rate are somehow related onto similar features and they are quite opposite of the high life expectancy at birth, expected years of schooling for children of school entering age, gross national income per capita and the proportion of females divided with proportion of males with at least secondary education. In other words, if we think about countries, those countries are more left which have higher education (also for girls), population health, and their people have more money to use. On the right are the countries which have higher mortality ratio for mother in childbirth and under aged girls are becoming mothers. On the top there are countries with larger number proportion of females in work life or having place in the parliament, while below part of the biplot there are less females than males in the parliament and work.


```{r PCA on the standardized human data}
# Perform principal component analysis on the standardized human data (with the SVD method)
pca_human_scaled<-prcomp(human_scaled)

# Create and print summary of pca_human
s_s<-summary(pca_human_scaled)
s_s

# Rounded percentages of variance captured by each PC, print them out
pca_pr2<-round(1*s_s$importance[2,]*100, digits=1)
pca_pr2

# Create object pc_lab to be used as axis labels
pc_lab<-paste0(names(pca_pr2), "(", pca_pr2, "%)")

# Draw a biplot for standardized data 
biplot(pca_human_scaled, cex=c(0.8,1), col=c("grey40", "deeppink2"), xlab=pc_lab, ylab=NA, main="Biplot using standardized human data")
```

Based on the biplot drawn after PCA on the standardized human data shows that the first two principal components PC1 is about how developed the country is (health, economical and educational point of view) and PC2 is mainly describing the role of the woman in the country (does women stay home or work and do they have a voice in country's political system). Because the angle between Parli.F or LaboFM is large between these two arrows and PC1, they are not correlated how developed the country is economically. From arrows it is possible to divide arrows into three different groups. Inside these groups the correlation between the variables (features) is rather small (small angle), but between the groups it is high (large angle).  Inside these groups, Parli.F and Labo.FM have the largest angle between those their arrows, which means that their relationship is less correlated than features inside other two groups. The length of the arrows is quite same, perhaps a bit smaller are the Edu2.FM and GNI. In addition, these two last mentioned features have the highest correlation between them (smallest angle). If we return to first correlation tests coming from correlation plot, it told us that highest correlation would be Edu.Exp and Life.Exp, which is not quite same but close what we get from the pca biplot and arrows. However, Mat.Mor and Life.Exp seems to be negatively correlated as they were in the correlation plot and Edu2.FM and Labo.FM have very close to 90 degrees angle between their arrows, which means that they are the least correlated arrows (features) in this analysis.

The tea data set is loaded from the package FactoMineR. In this data set is a questionnaire on tea, where 300 individuals were asked how they drink their tea with 18 questions. Besides that there were questions about their product's perception with 12 questions and 4 questions about their personal details. So there is 36 columns and 300 rows, rows are the individuals that answered the questionnaire and columns are the different questions. The first 18 questions are active, the age question is a supplementary quantitative variable and the last questions are supplementary categorical variables.


```{r The tea data}
# Access to FactoMineR (includes tea data) and dplyr (suppress warnings)
suppressPackageStartupMessages({
library("dplyr")
})
library(FactoMineR)
library(dplyr)

# The names of the datasets in the FactoMineR
data(package="FactoMineR")

# Load tea dataset from the package
data(tea)

# Summary and structure (questions) of the tea dataset
summary(tea)
str(tea)
```

In above there is a structure of tea data. It has the 300 objects and 36 variables as it should have. The questions are the columns in the data set and they are factors with 2 or more levels  meaning that the individuals need to choose the answer from given 2 or more options. Only difference is the age, where the answer needs to be integer.

If we first take a look at those 18 first questions about how individuals have their tea. The first group of bar plots is about these questions. Then next six are personal questions and about the age group and frequency of having tea, and lastly group of 12 bar plots are about the product's perception.

```{r Data divided into 3 groups}
# Dividing data for groups for analysis (tea_how, personal, perception)
keep_columns1<-c("breakfast", "tea.time", "evening", "lunch","dinner","always","home","work","tearoom","friends","resto","pub","Tea","How","sugar","how","where","price")
keep_columns2<-c("age","sex", "SPC","Sport","age_Q","frequency")
keep_columns3<-c("escape.exoticism", "spirituality", "healthy", "diuretic", "friendliness", "iron.absorption", "feminine", "sophisticated", "slimming", "exciting", "relaxing")

tea_how<-select(tea, one_of(keep_columns1))
tea_inf<-select(tea, one_of(keep_columns2))
tea_per<-select(tea, one_of(keep_columns3))

# visualize the how dataset (and suppress some warnings)
suppressWarnings(gather(tea_how)%>%ggplot(aes(value))+facet_wrap("key", scales="free")+geom_bar(fill="darkblue")+theme(axis.text.x=element_text(angle=45, hjust=1, size=7), axis.text.y=element_text(hjust=1, size=6), axis.title.x = element_blank(), axis.title.y=element_blank()))

suppressWarnings(gather(tea_inf)%>%ggplot(aes(value))+facet_wrap("key", scales="free")+geom_bar(fill="darkred")+theme(axis.text.x=element_text(angle=90, hjust=1, size=7), axis.text.y=element_text(hjust=1, size=6), axis.title.x = element_blank(), axis.title.y=element_blank()))

suppressWarnings(gather(tea_per)%>%ggplot(aes(value))+facet_wrap("key", scales="free")+geom_bar(fill="darkgreen")+theme(axis.text.x=element_text(angle=45, hjust=1, size=7), axis.text.y=element_text(hjust=1, size=6), axis.title.x = element_blank(), axis.title.y=element_blank()))

```

 Then do Multiple Correspondence Analysis on the tea data (or to a certain columns of the data, it’s up to you). Interpret the results of the MCA and draw at least the variable biplot of the analysis. You can also explore other plotting options for MCA. Comment on the output of the plots. 
```{r MCA on whole tea data} 
# MCA for tea_how, having 
mca_tea<-MCA(tea, quanti.sup=19, quali.sup=20:36, graph=F)

# Summary for the models 
summary(mca_tea)

# Visualize mca_tea
plot(mca_tea, invisible=c("ind"), cex=0.8, selectMod="contrib 20", unselect="grey30") 

# Another visualization of the mca_tea 
plot(mca_tea, choix="var", xlim=c(0,0.5), ylim=c(0,0.5), cex=0.7)
```
  
From the summary table one can read that the dimension 1 is explaining approximately 10% of the variance and the second dimension around 8%. Categorical variables are the squared correlation between each variable and dimension. If the value is close to 1, then there is a strong link between the current dimension and variable. From the summary table, the strongest link is between tearoom and dimension 1 (from 10 most correlated categorical variables). This is best  visualized in the second biplot, where tearoom stands on the far right on the x-axis.

In the first MCA biplot there is no group of individuals, only variables. In the x-axis, there is dimension 1, and Y-axis is dimension 2. The 20 most contributed variables are written in red color, other variables are just grey triangles in the MCA biplot.

The second biplot is coloring the variables so in red you can see the active variables, in green the supplementary categorical variables and in blue the supplementary continuous variables. The coordinate for a categorical variable on a dimension is squared correlation ratio between the dimension and the categorical variable, where as the coordinate for a continuous variable on a dimension is a squared correlation coefficient between the dimension and the continuous variable. The distance between variable categories gives a measure in their similarity, so in the last plot it is seen that resto and friends are closer than Tea and friends.


  






  
  