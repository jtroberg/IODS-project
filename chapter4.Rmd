# Week 4

In this week's Rstudio exercise we will use the Boston data set. It has 506 objects and 14 different variables, so the dataset has 506 rows and 14 columns.

The columns in Boston data set are:

1. crim: per capita crime rate by town
2. zn: proportion of residential land zoned for lots over 25,000 sq.ft
3. indus: proportion of non-retail business acres per town
4. chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
5. nox: nitrogen oxides concentration (parts per 10 million)
6. rm: average number of rooms per dwelling
7. age: proportion of owner-occupied units built prior to 1940
8. dis: weighted mean of distances to five Boston employment centres
9. rad: index of accessibility to radial highways
10. tax: full-value property-tax rate per \$10,000
11. ptratio: pupil-teacher ratio by town
12. black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town 
13. lstat: lower status of the population (percent)
14. medv: median value of owner-occupied homes in \$1000s



```{r}
# Load the Boston data from MASS package
library(MASS)
data("Boston")

# Explore the structure and dimension of the data
str(Boston)
dim(Boston)
```

The structure view in table above shows that Boston data contains only numerical values.

In the plot below, the linear correlation between variables is roughly determined with scatterplot matrix. In the diagonal line from top left to bottom right there are Boston data variables and then each variable is plotted against each other.

In the next data table there are summaries of the variables. It seems that some of the variables have quite large distribution, such as crim and zn, while others are quite similar, ptratio for example. The values vary from 0 to 711, so they need to be scaled, if we wish to handle them simultaniously in the analysis. 

Below these the correlation of the variables are more examined with correlation plot and visualized plot of this data. The visualized correlation plot shows nicely which variables are correlated and which are not.

```{r}
# Access to dplyr and corrplot
library(dplyr)
library(corrplot)

# Scatterplot matrix of the variables
pairs(Boston)

# Summaries of the variables
summary(Boston)

# For correlation, a "nicer"" correlation matrix is made wit corrplot 
cor_matrix<-cor(Boston)

# Print the correlation matrix (values rounded, first 2 digits)
cor_matrix%>%round(2)

# Visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)

```

The data is next standardized. The variables changed now so that values are now variating from -3.9 to 9.9. Standardization is done in the scaling so that the column means are substracted from their columns and the resulting difference is then divided with standard deviation.

```{r}
# Center and standardize variables
boston_scaled<-scale(Boston)

# Summaries of the scaled variables
summary(boston_scaled)

# Boston_scale is changed to data frame format
boston_scaled<-as.data.frame(boston_scaled)

# create a categorical variable of the crime rate 
# Creat scaled_crim
scaled_crim<-boston_scaled$crim

# create a quantile vector of crim
bins<-quantile(scaled_crim)

# create the categorical variable ´crime´
crime<-cut(scaled_crim, breaks=bins, include.lowest=TRUE, label=c("low", "med_low", "med_high", "high"))

# remove the original crim from the dataset
boston_scaled<-dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled<-data.frame(boston_scaled, crime)

# splitting the Boston data set to test and train sets
# number of rows in data set
n<-nrow(boston_scaled)

# choose randomly 80% of the rows
ind<-sample(n, size=n*0.8)

# create train and test sets
train<-boston_scaled[ind,]
test<-boston_scaled[-ind,]
```

The linear discriminant analysis is then fit to the train set. The categorical crime rate is the target variable and all the other variables in the dataset are predictor variables.

```{r}
# LDA on the train set, categorical crime rate as target variable, all other variables as predictor variables.
lda.fit<-lda(crime~., data=train)

# target class as numeric
classes<-as.numeric(train$crime)

# draw the LDA (bi)plot, arrows added
plot(lda.fit, dimen=2, col=classes, pch=classes)

```

Next the crime categories are saved to test data and then the categorical crime variable is removed from the test dataset. After this prediction of the classes is made with LDA model and results are cross tabulated with the crime categories from the test set.
From the cross tabulation, the prediction of teh model (prediction as probabilities), it seems that high prediction is the most accurate one and next is the low. Then the medium ones are not that well predicted.

```{r}
# save the crime categories from the test data
correct_classes<-test$crime

# remove the categorical crime from test data
test<-dplyr::select(test, -crime)

# precict classes with test data
lda.pred<-predict(lda.fit, newdata=test)

# cross tabulate the results
table(correct=correct_classes, predicted=lda.pred$class)
```

Lastly, the distances between the observations are calculated and k-means algorith is run on the dataset. 
```{r}
# reload Boston data set
data(Boston)

# center and standardize variables
boston_scaled<-scale(Boston)

# calculate the distances between observations
dist_eu<-dist(Boston)

# k-means clustering
km<-kmeans(dist_eu, centers=15)

# plot the dataset with clusters
pairs(Boston, col=km$cluster)
```

Based on the plot, the optimal number of clusters is 2, because then the total WCSS drops radically in the figure and the algorithm is rerun with this. The clusters are then visualized with a plot where clusters are separated with colors and results then interpreted. 

```{r}
# set.seed to prevent producing different results everytime
set.seed(123)

# calculate the distances between observations
dist_eu<-dist(Boston)

# determine the number of clusters
k_max<-10

# calculate the total WCSS
twcss<-sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})

# visualize the results
plot(1:k_max, twcss, type='b')

# k-means clustering
km<-kmeans(dist_eu, centers=2)

# plot the dataset with clusters
pairs(Boston, col=km$cluster)
```

